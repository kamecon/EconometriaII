---
title: "Procesos Estocásticos y Series Temporales"
subtitle: "Econometría II"
author: "Kamal Romero"
date: "(actualizado: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
```

```{r librerias, echo = FALSE, warning=FALSE, message=FALSE}
if (! ('pacman' %in% installed.packages())) install.packages('pacman')
pacman::p_load(tidyverse, kableExtra, quantmod, fredr, patchwork, fpp2, gridExtra)
```

## Tipo de Cambio Nominal

- El comportamiento del tipo de cambio es lejos de ser sistemático y predecible

```{r er0librerias, echo = FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Cargamos los datos
load("data/euro_dolar.RData")
load("data/euro_libra.RData")
load("data/euro_yen.RData")

```

```{r er1, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

plot(index(euro_dolar),as.numeric(euro_dolar[,6]),typ='l',xlab='',ylab='EUR/USD', main = "Tipo de Cambio Euro/Dólar")


```

---

## Tipo de Cambio Nominal

- El comportamiento del tipo de cambio es lejos de ser sistemático y predecible

```{r er2, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

plot(index(euro_libra),as.numeric(euro_libra[,6]),typ='l',xlab='',ylab='EUR/GBP', main = "Tipo de Cambio Euro/Libra")


```

---

## Tipo de Cambio Nominal

- El comportamiento del tipo de cambio es lejos de ser sistemático y predecible

```{r er3, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

plot(index(euro_yen),as.numeric(euro_yen[,6]),typ='l',xlab='',ylab='EUR/JPY', main = "Tipo de Cambio Euro/Yen")


```

---

## Tipo de Cambio Nominal

- Aunque sus variaciones presentan un comportamiento más predecible

```{r er4, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

plot(index(euro_dolar),as.numeric(euro_dolar[,7]),typ='l',xlab='',ylab='EUR/USD', main = "Variación del Tipo de Cambio Euro/Dólar")

```

---

## Tipo de Cambio Nominal

- Aunque sus variaciones presentan un comportamiento más predecible

```{r er5, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

plot(index(euro_libra),as.numeric(euro_libra[,7]),typ='l',xlab='',ylab='EUR/GBP', main = "Variación del Tipo de Cambio Euro/Libra")

```

---

## Tipo de Cambio Nominal

- Aunque sus variaciones presentan un comportamiento más predecible

```{r er6, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

plot(index(euro_yen),as.numeric(euro_yen[,7]),typ='l',xlab='',ylab='EUR/USD', main = "Variación del Tipo de Cambio Euro/Yen")

```

---

## Proceso estocástico y serie temporal

- Una serie temporal es una colección de observaciones de una variable tomadas de forma secuencial y ordenada en el tiempo (instantes de tiempo equiespaciados). Las series pueden tener una periodicidad anual, semestral, trimestral, mensual, semanal, diaria etc., según los periodos de tiempo en los que están recogidos los datos que la componen.

- Un proceso estocástico es una secuencia de variables aleatorias, ordenadas y equidistantes cronológicamente referidas a una característica observable en diferentes momentos.

- Por ello, una serie temporal es una **realización particular de una muestra procedente de un proceso estocástico.**


---

## Proceso estocástico y serie temporal

- Una serie temporal es una realización particular de un proceso estocástico

```{r proceso01, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

# Parámetros
phi= -0.7048                                 
y=rep(0,100)
cons=2.1                                     
y[1]=4.1

#Creamos un data frame vacio que luego rellenamos con distintas realizaciones de series temporales
datos <- matrix(0, nrow = 100, ncol = 10) %>% data.frame()

for (i in 1:10) {
  datos[1,i]=4.1
  e=rnorm(n=100,0,0.7) 
  for (j in 2:100) datos[j,i]=cons+phi*datos[j-1,i]+e[j]   
}

plot(datos[,1] %>% ts(), col=2, lwd=3, main="Proceso Estocástico", xlab="Tiempo", ylab="Serie")

```

---

## Proceso estocástico y serie temporal

- Una serie temporal es una realización particular de un proceso estocástico

```{r proceso02, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

plot(datos[,1] %>% ts(), col=2, lwd=3, main="Proceso Estocástico", xlab="Tiempo", ylab="Serie")
lines(datos[,2] %>% ts(), col=3)

```


---

## Proceso estocástico y serie temporal

- Una serie temporal es una realización particular de un proceso estocástico

```{r proceso03, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

plot(datos[,1] %>% ts(), col=2, lwd=3, main="Proceso Estocástico", xlab="Tiempo", ylab="Serie")
lines(datos[,2] %>% ts(), col=3)
lines(datos[,3] %>% ts(), col=4)

```


---

## Proceso estocástico y serie temporal

- Una serie temporal es una realización particular de un proceso estocástico

```{r proceso04, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

plot(datos[,1] %>% ts(), col=2, lwd=5, main="Proceso Estocástico", xlab="Tiempo", ylab="Serie")
for (i in 2:10) {
  lines(datos[,i] %>% ts(), col=i+1)
}

```


---

## Proceso estocástico y serie temporal

<img src="proceso01.jpg" width="700">


---

## Proceso estocástico y serie temporal
 
### Distribución de un Proceso Estocástico

- En analogía con las variables aleatorias (resultado de lanzar una moneda) queremos introducir caracteristicas no aletorias de los procesos estocásticos tales como su distribución, su esperanza, varianza, etc, y describir su **estructura de dependencia**.

- Esta es una tarea mucho más complicada que en el caso de vectores de variables aleatorias (lanzar una moneda *n* veces).

- De hecho un proceso estocástico no-trivial con un conjunto índice $T$ es un objeto de dimensión infinita en el sentido de que se puede entender como una colección infinita de variables aleatorias.

- En este curso intentaremos algo mucho mas simple.

---


## Proceso estocástico y serie temporal


  - Elaborar un **modelo estadístico** para una muestra procedente de un proceso estocástico $Y_t$ a partir de una única realización particular (una serie temporal $y_t$).
  
  - Utilizar el modelo elaborado para **prever** los futuros valores de proceso $Y_{(N+l)}$.
  
  - **Contrastar** alguna teoría sobre la característica o variable a la que se refiere la serie considerada.
  

---


## Proceso estocástico y serie temporal

### ¿Cómo lo vamos a hacer?

Al igual que en la Econometría básica trabajábamos con dos los supuestos de *i.i.d.* (independiente e idénticamente distribuido), en la Econometría de Series Temporales nos hace faltan dos supuestos equivalentes:

- <span style="color:red">**Estacionariedad**</span> (substituye al supuesto de identicamente distribuido)

- <span style="color:red">**Ergodicidad**</span> (substituye al supuesto de independencia)


  
---


## Proceso estocástico y serie temporal

### ¿Cómo lo vamos a hacer?

- Para conseguir estos objetivos se va a seleccionar un modelo para la muestra dentro de una clase general de modelos denominados ARIMA, que implique cierta propiedades teóricas para el proceso estocástico $Y_t$ del que procede la muestra y que resulten compatibles con las propiedades muestrales observadas en la serie temporal $y_t$.

- El problema para hacer esto es que solo tenemos una única realización particular del proceso estocástico. Para solucionar esto necesitamos que se cumpla la hipótesis de estacionariedad mencionado antes.
  
  
---

## Estacionariedad: Definición


Si $\{y_t\}$ es una serie temporal estacionaria, entonces para todo $s$, la distribución de $(y_t,\dots,y_{t+s})$ no depende de $t$.


Una **serie estacionaria** es:

*  aproximadamente horizontal
*  varianza constante
*  no hay patrones predecibles a largo plazo 
---

## ¿Estacionario?

```{r fig.width=12, fig.height=7.5}
autoplot(dj) + ylab("Dow Jones Index") + xlab("Día")
```

---

## ¿Estacionario?

```{r fig.width=12, fig.height=7.5}
autoplot(diff(dj)) + ylab("Cambio en el Dow Jones Index") + xlab("Día")
```

---


## ¿Estacionario?
```{r fig.width=12, fig.height=7.5}
autoplot(eggs) + xlab("Año") + ylab("$") +
  ggtitle("Precio de una docena de huevos en dólares de 1993")
```

---


## ¿Estacionario?

```{r fig.width=12, fig.height=7.5}
autoplot(window(ausbeer, start=1992)) + xlab("Año") + ylab("megalitros") +
  ggtitle("Producción trimestral de cerveza en Australia")
```


---

## Estacionariedad: Definición

Vamos a trabajar este concepto desde su definición más simple a una con mayor complejidad

Si $\{y_t\}$ es una serie temporal estacionaria, entonces para todo $s$, la distribución de $(y_t,\dots,y_{t+s})$ no depende de $t$.


Una **serie estacionaria** es:

*  aproximadamente horizontal
*  varianza constante
*  no hay patrones predecibles a largo plazo 

---

## Estacionariedad: Definición (más formal)

Un proceso se dice que es **estacionario debil** de orden *n* si todos sus momentos conjuntos de orden *n* existen y son invariantes en el tiempo.

Procesos Estacionarios en Covarianzas (de 2º orden):

- Esperanza constante

- Varianza constante

- La función de covarianzas depende solo de la diferencia temporal entre las variables


---

## Estacionariedad: Definición (aún más formal)

- Un proceso estocástico $(Y_t)$ es **estrictamente estacionario** si y sólo si para cualesquiera $n \geq 1$ momentos $t_1<t_2< \ldots t_n$ de su historia, la distribución de probabilidad conjunta de $[Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n}]'$ coincide con la de $[Y_{t_1+h}, Y_{t_2+h}, \ldots, Y_{t_n+h}]'$ para cualquier número entero $h=\pm 1, \pm 2, \ldots$ (distinto de cero).

---

## Estacionariedad: Definición (aún más formal)

- Un proceso estocástico $(Y_t)$ con $E(Y_t) < \infty$ para todo $t=\pm 1, \pm 2, \ldots$ es **estacionario en media** o **débilmente estacionario de primer orden** si y solo si $E(Y_t)$ es constante (no depende de $t$) para todo $t=\pm 1, \pm 2, \ldots$.

- Un proceso estocástico $(Y_t)$ con $E(Y^2_t) < \infty$ para todo $t= \pm 1, \pm 2, \ldots$ es **estacionario en autocovarianza** o **débilmente estacionario de segundo orden** si y solo si:

   - $E(Y_t)$ y $Var(Y_t)$ son constantes (no depende de $t$) para todo $t= \pm 1, \pm 2, \ldots$.

   - $Cov(Y_t, Y_{t+k})$ depende a lo sumo de $k$ pero no de $t$ para todo $t=\pm 1, \pm 2, \ldots$.

---

## Estacionariedad: Definición (aún más formal)

- Un proceso estocástico $(Y_t)$ es **normal** o **gaussiano** cuando para cualesquiera $n \geq 1$ momentos $t_1<t_2< \ldots t_n$ de su historia, la distribución de probabilidad conjunta de $[Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n}]'$ es una distribución normal $n-$variante.

> <span style="color:red"> Estacionariedad en autocovarianza + Normalidad $\Rightarrow$ Estacionariedad Estricta </span>

---


## Propiedades  de la hipótesis de estacionariedad

   - <span style="color:red">**Estacionariedad**</span>: cierto estado de equilibrio estadístico que caracteriza la evolución temporal de un proceso estocástico que ha genera una serie temporal.
   
   -  <span style="color:red">**Media**</span>: valor constante en el tiempo que mide el nivel alrededor del cual evoluciona un proceso estocástico estacionario.
   
   -  <span style="color:red">**Varianza**</span>: valor constante en el tiempo que mide la dispersión o la variabilidad de la evolución temporal de un proceso estocástico estacionario alrededor de su media.
   
   -  <span style="color:red">**Autocorrelaciones**</span>: valores constantes en el tiempo que miden el grado de asociación lineal entre cada par de componentes de un proceso estocástico estacionario separados por diferentes intervalos temporales o retardos.

---

## Autocorrelación

**Covarianza** y **correlación**: miden el grado de **relación lineal** entre dos variables $y$ y $X$.

**Autocovarianza** y **autocorrelación**: miden la relación lineal entre los **valores retardados** de una serie temporal $y$.

Medimos la relación entre:

  * $y_{t}$ y $y_{t-1}$
  * $y_{t}$ y $y_{t-2}$
  * $y_{t}$ y $y_{t-3}$
  * etc.

---

## Autocorrelación

Denotamos la autocovarianza muestral con retardo $k$ por $c_k$ y la autocorrelación muestral con retardo $k$ por $r_k$.  Entonces definimos

$$c_k = \frac{1}{T}\sum_{t=k+1}^T (y_t-\bar{y})(y_{t-k}-\bar{y})$$

y

$r_{k} = c_k/c_0$


  * $r_1$ indica cómo se relacionan los valores sucesivos de $y$
  * $r_2$ indica cómo se relacionan los valores de $y$ con dos períodos de diferencia
  * $r_k$ es *casi* lo mismo que la correlación muestral entre $y_t$ y $y_{t-k}$.

---

## Autocovarianzas teóricas

- La **autocovarianza de orden $k$** $(k > 0)$ de un proceso $(Y_t)$ estacionario se representa con el símbolo $\gamma_k$ y se define como $\gamma_k \equiv Cov(Y_t, Y_{t+k})$.

$$\begin{array}{|cc|}
\hline
Media  & \mu_Y \equiv E(Yt) \\
Varianza:  & \sigma^2_Y \equiv Var(Y_t) \equiv E[(Y_t - \mu_Y)^2]  \\
Autocov. orden\; k:  & \gamma_k \equiv Cov(Y_t, Y_{t+k}) \equiv E[(Y_t - \mu_Y)(Y_{t+k} - \mu_Y)](k=1,\ldots) \\
\hline
\end{array}$$


- La autocovarianza de orden $k$ de $(Y_t)$ es la covarianza entre cualquier par de componentes de $(Y_t)$ separados entre sí por un intervalo temporal o **retardo** $(k > 0)$ dado.

- La autocovarianza de orden cero como $\gamma_k \equiv Cov(Y_t, Y_t) \equiv Var(Y_t)$, que es la varianza del proceso estacionario $(Y_t)$   

- La secuencia $(\gamma_k:k =0, 1, 2, \ldots)$ se denomina **la función de autocovarianza** del proceso estacionario $(Y_t)$.


---

## Autocorrelaciones simples teóricas

- La **autocorrelación simple de orden $k$** $(k>0)$ de un proceso $(Y_t)$ estacionario se representa con el símbolo $\rho_k$ y se define como:

$$\rho_k \equiv \frac{Cov(Y_t, Y_{t+k})}{Var(Y_t)^{1/2}Var(Y_{t+k})^{1/2}} \equiv \frac{\gamma_k}{\gamma_0}$$


$$\begin{array}{|cc|}
\hline
Autocorrelacion\; simple\; de\; orden \; k:  & \displaystyle\frac{\gamma_k}{\gamma_0} (k=1,2,\ldots); \; \rho_0=1 \\
\hline
\end{array}$$

- Considerada como una función de $k$, $(\rho_k: k = 1, 2, \ldots)$ se denomina **la función de autocorrelación simple (ACF)** del proceso estacionario $(Y_t)$. Dado que cada $\rho_k$ es un coeficiente de correlación, suele decirse que la ACF de $(Y_t)$ represente la **duración** y la **intensidad** de la **memoria** del proceso $(Y_t)$.

---

## PACF teórica


- La **autocorrelación parcial de orden $k$** de un proceso $(Y_t)$ estacionario se define como el parámetro $\phi_{kk}$ en la regresión:

$$\tilde{Y_t} = \phi_{k0} + \phi_{k1}\tilde{Y}_{t-1} + \phi_{k2}\tilde{Y}_{t-2} + \ldots + \phi_{kk}\tilde{Y}_{t-k} + U_t$$

- Donde $\tilde{Y}_{t-i} \equiv Y_{t-i} - \mu_Y \; (i=0,1,\ldots,k)$ y $U_t$ es independiente de $Y_{t-i}$ para todo $i \geq 1$

- Además, $\triangle Y_t = \phi_{kk} \times \triangle Y_{t-k}$ cuando $\triangle Y_{t-1} = \triangle Y_{t-2} = \ldots = \triangle Y_{t-k+1} = \triangle U_{t} =0$.

- $\phi_{kk}$ representa el **efecto parcial** o **ceteris paribus** de $\triangle Y_{t-k}$ sobre $\triangle Y_{t}$.

- $\phi_{kk}$ es una medida del grado de asociación lineal entre dos componentes cualesquiera de $(Y_t)$ separados entre sí por el retardo $k$ dado, que **no** es debida a la posible correlación entre los componentes de $(Y_t)$ que se encuentran entre ambos.


---

## PACF teórica

- A partir de la regresión anterior, puede comprobarse que $\phi_{kk}=|A_k|/|B_k|$, donde

$$A_k \equiv \left(
               \begin{array}{cccccc}
                 1 & \rho_1 & \rho_2 & \cdots & \rho_{k-2} & \rho_1 \\
                 \rho_1 & 1 & \rho_1 & \cdots & \rho_{k-3} & \rho_2 \\
                 \rho_2 & \rho_1 & 1 & \cdots & \rho_{k-4} & \rho_3 \\
                 \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
                 \rho_{k-1} & \rho_{k-2} & \rho_{k-3} & \cdots & \rho_{1} & \rho_k \\
               \end{array}
             \right)$$
             
$$B_k \equiv \left(
               \begin{array}{cccccc}
                 1 & \rho_1 & \rho_2 & \cdots & \rho_{k-2} & \rho_{k-1} \\
                 \rho_1 & 1 & \rho_1 & \cdots & \rho_{k-3} & \rho_{k-2} \\
                 \rho_2 & \rho_1 & 1 & \cdots & \rho_{k-4} & \rho_{k-3} \\
                 \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
                 \rho_{k-1} & \rho_{k-2} & \rho_{k-3} & \cdots & \rho_{1} & 1 \\
               \end{array}\right)$$


---

## PACF teórica

- En particular, para $k=1,2,3$ resulta que:

$$\phi_{11} = \rho_1 , \;\; \phi_{22} = \frac{\left|\begin{array}{cc}
                                             1 & \rho_1 \\
                                             \rho_1 & \rho_2 \\
                                           \end{array}\right|}{\left|\begin{array}{cc}
                                             1 & \rho_1 \\
                                             \rho_1 & 1 \\
                                           \end{array}\right|} = \frac{\rho_2-\rho^2_1}{1-\rho_1^2}$$

---

 ## PACF teórica

- Lo que muestra la anterior expresión es que cada coeficiente de autocorrelación parcial $\phi_{kk}$ de un proceso estrictamente estacionario es una función concreta de los coeficientes de autocorrelación simple $\rho_1, \rho_2, \ldots, \rho_k$ $(k=1,2,3, \ldots)$.

- Considerada como una función del retardo $k$, la secuencia $(\phi_{kk}:k=1,2,\ldots)$ se denomina **la función de autocorrelación parcial (PACF)**

$$\begin{array}{|cc|}
\hline
Autocorrelación\; parcial\; de\; orden\; k: & \phi_{kk}= \frac{|A_k|}{|B_k|} (k=1,2,\ldots); \phi_{11} = \rho_1 \\
\hline
\end{array}$$

---

## Problema de estimación


- Si una muestra $Y \equiv [Y_1, Y_2, \ldots, Y_N]'$ **no procede de un proceso estacionario**, entonces su vector de medias y su matriz de varianzas-covarianzas son:
 
$${\mu} \equiv E(Y) = \left(
                        \begin{array}{c}
                          \mu_1 \\
                          \mu_2 \\
                          \vdots \\
                          \mu_N \\
                        \end{array}
                      \right)$$
 
 
$$\Sigma \equiv Var(Y) = \left(
                          \begin{array}{cccc}
                             \sigma^2_1 & \sigma_{12} & \cdots & \sigma_{1N} \\
                             \sigma_{21} & \sigma^2_2 & \cdots & \sigma_{2N} \\
                             \vdots & \vdots & \ddots & \vdots \\
                             \sigma_{N1} & \sigma_{N2} & \cdots & \sigma^2_N \\
                           \end{array}\right)$$
                       
                       
- Las anteriores expresiones contienen $N+N(N+1)/2$ parámetros distintos. No pueden estimarse con precisión este número de parámetros utilizando tan solo una única serie temporal $y \equiv (y_1, y_2, \ldots, y_N)'$ de $N$ observaciones. 

- Si utilizamos una serie de tiempo que procede de un proceso estacionario, reducimos el número de parámetros a estimar

---

## Problema de estimación


- Si una muestra $Y \equiv [Y_1, Y_2, \ldots, Y_N]'$ procede de un proceso estacionario, entonces su vector de medias y su matriz de varianzas-covarianzas son:
 
$${\mu} \equiv E(Y) = \left(
                        \begin{array}{c}
                          \mu_Y \\
                          \mu_Y \\
                          \vdots \\
                          \mu_Y \\
                        \end{array}
                      \right)$$
 
$$\Sigma \equiv Var(Y) = \left(
                          \begin{array}{cccc}
                             \gamma_0 & \gamma_1 & \cdots & \gamma_{N-1} \\
                             \gamma_1 & \gamma_0 & \cdots & \gamma_{N-2} \\
                             \vdots & \vdots & \ddots & \vdots \\
                             \gamma_{N-1} & \gamma_{N-2} & \cdots & \gamma_0 \\
                           \end{array}\right)$$

                       
 
---

## Problema de estimación

$$\Sigma =\sigma^2_Y\left(
                \begin{array}{cccc}                                                                                                        1 & \rho_1 & \cdots & \rho_{N-1} \\
                  \rho_1 & 1 & \cdots & \rho_{N-2} \\
                  \vdots & \vdots & \ddots & \vdots \\
                  \rho_{N-1} & \rho_{N-2} & \cdots & 1 \\
                \end{array}\right)$$
                                                                                              
- Las anteriores expresiones contienen $1+N$ parámetros distintos. A pesar que la hipótesis de estacionariedad, no pueden estimarse con precisión $1+N$ parámetros utilizando tan solo una única serie temporal $y \equiv (y_1, y_2, \ldots, y_N)'$ de $N$ observaciones.

- La solución a este problema pasa por expresar la media y la función de autocovarianza de $Y_t$ en términos de un número reducido de parámetros, a través del uso de un **modelo ARMA** para $(Y_t)$.


---

## ACF y PACF muestrales


- En la práctica es imposible estimar a partir de una serie temporal de $N$ observaciones, la media, la varianza, la ACF y PACF del proceso estacionario del que supuestamente procede dicha serie.

- Sin embargo, sí pueden estimarse con cierta precisión las $K$ primeras autocorrelaciones simples y parciales del proceso estocástico a través de una serie temporal de $N$ observaciones, siempre que $K$ sea un número bastante más reducido que $N$.

- La **media muestral** y la **varianza muestral** de una muestra asociada con una serie temporal son:

$$\widehat{\mu}_Y \equiv \frac{1}{N}\sum_{t=1}^N Y_t\;\;\; \text{y} \;\;\;\widehat{\sigma}_Y^2 \equiv \frac{1}{N} \sum_{t=1}^N (Y_t-\widehat{\mu}_Y)^2$$



---

## ACF muestral

- La **correlación simple muestral de orden $k$** $(k>0)$ de una muestra asociada a una serie temporal es:
$$\widehat{\rho}_k \equiv \frac{\widehat{\gamma}_k}{\widehat{\gamma}_0},(k=1,2,\ldots)$$

donde:

$$\begin{array}{rcl}
\widehat{\gamma}_k & \equiv & \displaystyle\frac{1}{N} \displaystyle\sum_{t=1}^{N-k} (Y_t-\widehat{\mu}_Y)(Y_{t+k}-\widehat{\mu}_Y)\\
                   & \equiv & \displaystyle\frac{1}{N} \displaystyle\sum_{t=k+1}^N (Y_{t-k}-\widehat{\mu}_Y)(Y_t-\widehat{\mu}_Y) \; (k=0,1,2,\ldots)\\
                   \end{array}$$

---

## ACF muestral

- Los estimadores $\widehat{\gamma}_k$ y $\widehat{\rho}_k$ proporcionan **estimaciones** numéricas $c_k$ y $r_k$, respectivamente,

$$c_k \equiv \frac{1}{N} \sum_{t=1}^{N-k} (y_t-\hat{y})(y_{t+k}-\hat{y}) \equiv \frac{1}{N} \sum_{t=k+1}^{N} (y_{t-k}-\hat{y})(y_{t}-\hat{y}) \; (k=0,1,2,\ldots)$$

$$r_k = \frac{c_k}{c_0} \; (k=1,2,\ldots)$$

---

## ACF muestral

- La secuencia de valores numéricos $(r_k:k=1,2,\ldots)$ se denomina la **ACF muestral** de la serie temporal $y_1, y_2, \ldots, y_N$. La representación gráfica de la ACF muestral de una serie se denomina el **correlograma** de dicha serie.

- Bajo ciertas condiciones, $(\hat{\rho}_k) \sim NIID(0,1/N)$, de manera que cualquier autocorrelación simple $\rho_k$ $(k \geq 1)$ de $(Y_t)$ puede considerarse **individualmente** significativa al 5% cuando $|r_k| > 1.96 / \sqrt{N}$.

---

## Autocorrelación: Ejemplo (estacionalidad)

```{r fig.width=12, fig.height=7.5}
autoplot(window(ausbeer, start=1992)) + xlab("Año") + ylab("megalitros") +
  ggtitle("Producción trimestral de cerveza en Australia")
```


---

## Autocorrelación: Ejemplo (estacionalidad)

Resultados de los 9 primeros retardos de los datos de la producción de cerveza:



```{r, echo=FALSE}
beeracf <- matrix(acf(c(window(ausbeer, start=1992)), lag.max=9,
                      plot=FALSE)$acf[-1,,1], nrow=1)
colnames(beeracf) <- paste("r",1:9,sep="")
knitr::kable(beeracf, booktabs=TRUE, format = "html",
             align="c", digits=3, 
             format.args=list(nsmall=3))
```

```{r beeracf, fig.width=10, fig.height=5, fig.align='center'}
ggAcf(window(ausbeer, start=1992)) + labs(title = "Producción de cerveza en Australia")
```

---

## Autocorrelación: Ejemplo (estacionalidad)

  * $r_{4}$ más alto que para los otros retardos. Esto se debe al **patrón estacional de los datos**: los picos tienden a estar separados por **4 trimestres** y los mínimos tienden a estar separados por **2 trimestres**.
  * $r_2$ es más negativo que para los otros retardos porque los mínimos tienden a estar 2 trimestres por detrás de los máximos.
  * Juntos, las autocorrelaciones en los retardos $1, 2, \dots$, constituyen la **autocorrelación** o ACF.
  * El gráfico se conoce como **correlograma**.


```{r, fig.align='center', fig.width=10, fig.height=3.5}
ggAcf(window(ausbeer, start=1992)) + labs(title = "Producción de cerveza en Australia")
```
 
---


## Tendencia y estacionalidad en los gráficos ACF

- Cuando los datos tienen una tendencia, las autocorrelaciones para retardos pequeños tienden a ser grandes y positivas.

- Cuando los datos son estacionales, las autocorrelaciones serán mayores en los retardos estacionales (es decir, en los múltiplos de la frecuencia estacional)

- Cuando los datos tienen tendencia y son estacionales, se observa una combinación de estos efectos


---

## Autocorrelación: Ejemplo (tendencia)

### Precio de la acción de Google

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7}
autoplot(goog)  + labs(title = "Precio de la acción de Google")
```


---

## Autocorrelación: Ejemplo (tendencia)

### Precio de la acción de Google


```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6}
ggAcf(goog, lag.max=100) + labs(title = "Precio de la acción de Google")
```

- Cuando los datos tienen una tendencia, las autocorrelaciones para retardos pequeños tienden a ser grandes y positivas.

---

## Producción mensual de electricidad en Australia

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7}
elec2 <- window(elec, start=1980)
autoplot(elec2) + labs(title = "Producción mensual de electricidad en Australia")
```

---

## Producción mensual de electricidad en Australia

```{r , warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7}
ggAcf(elec2, lag.max=48) + labs(title = "Producción mensual de electricidad en Australia")
```

---

## Producción mensual de electricidad en Australia

El gráfico muestra una clara tendencia y estacionalidad.

Las mismas características se reflejan en el ACF.

  * El ACF, que decae lentamente, indica una tendencia.
  
  * Los picos del ACF en los retardos 12, 24 y 36, indican una estacionalidad de longitud 12.
  
---

## Proceso ruido blanco

- Ruido blanco $x_t = w_t$

- Donde $E(w_t)=0\;\;\; var(w_t)=\sigma^2 \;\;\; \forall t$

```{r ruido01, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7}

w = rnorm(500,0,1)

plot(w %>% ts, main="Ruido Blanco", xlab="Tiempo", ylab="Serie")

```


---

## Proceso ruido blanco

- Lo vemos con menos datos para que se visalize mejor

```{r ruido02, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

w1 = rnorm(100,0,1)

plot(w1 %>% ts, main="Ruido Blanco", xlab="Tiempo", ylab="Serie", type="b", col=2)

```


---

## Proceso ruido blanco

```{r, echo=FALSE}
wracf <- matrix(acf(c(w1), lag.max=9,
                      plot=FALSE)$acf[-1,,1], nrow=1)
colnames(wracf) <- paste("r",1:9,sep="")
knitr::kable(wracf, booktabs=TRUE,
             align="c", digits=3,
             format.args=list(nsmall=3))
```

```{r wnacfwarning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=4}
ggAcf(w1) + labs(title = "Proceso ruido blanco")
```

- Autocorrelación muestral para procesos ruido blanco 

- Esperamos que cada autocorrelación sea cercana a cero


---

## Proceso ruido blanco con deriva (drift)

- Ruido blanco con deriva $x_t =\delta +  w_t$

```{r ruido03, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

delta = 2
ww = w + delta
plot(ww %>% ts, main="Ruido Blanco con deriva", xlab="Tiempo", ylab="Serie", type="l", col=1)

```

---

## Proceso media movil

- Media móvil $x_t =  \theta w_t$

- En este caso particular el proceso es $x_t =\frac{1}{3} (w_{t-1}+w_t+w_{t+1})$. Se puede interpretar como una media centrada en cada observación


```{r ma01, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6.5}

v = stats::filter(w, sides=2, rep(1/3,3))

plot(v %>% ts, main="Media Móvil", xlab="Tiempo", ylab="Serie", type="l", col=1)

```

---

## Ambos procesos

```{r ma02, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

par(mfrow=c(2,1))
plot(w %>% ts, main="Ruido Blanco", xlab="Tiempo", ylab="Serie", type="l", col=1)
plot(v %>% ts, main="Media Móvil", xlab="Tiempo", ylab="Serie", type="l", col=2)

```

---

## Proceso media movil: ACF


```{r maacf, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6}
ggAcf(v) + labs(title = "Proceso media movil")
```

---


## Proceso media movil: ACF

$$x_t=w_t+0.5w_{t-1}+0.24w_{t-2}$$

```{r maacf02, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6}
b1 <- 0.5
b2 <- 0.24
v2 <- arima.sim(list(ma = c(b1,b2)), n=100, sd=2)
ggAcf(v2) + labs(title = "Proceso media movil")
```

---

## Proceso media movil: ACF

$$x_t=w_t+0.3w_{t-1}$$

```{r maacf03, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6}
v3 <- arima.sim(list(ma = c(0.3)), n=100, sd=2)
ggAcf(v3) + labs(title = "Proceso media movil")
```

---


## Procesos autoregresivos 

- Proceso autoregresivo de orden 2 $x_t = x_{t-1} - 0.9x_{t-2} + w_t$

```{r ar01, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

phi= -0.9          
z=rep(0,500)                                 
z[1]=0
z[2]=2
for (i in 3:500) z[i]=z[i-1]+phi*z[i-2]+w[i]   
plot(ts(z), main="Proceso autoregresivo", xlab="tiempo", ylab="Serie")  

```


---

## Procesos autoregresivos 

- Proceso autoregresivo de orden 2 $x_t = x_{t-1} - 0.9x_{t-2} + w_t$

```{r aracf, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6}
ggAcf(z) + labs(title = "Proceso autoregresivo")
```


---

## Procesos autoregresivos 

- Proceso autoregresivo de orden 2 $x_t = x_{t-1} - 0.9x_{t-2} + w_t$

```{r arpacf1, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6}
ggPacf(z) + labs(title = "PACF Proceso autoregresivo")
```


---

## Procesos autoregresivos 

- Proceso autoregresivo de orden 1 $x_t = 0.8x_{t-1} + w_t$

```{r ar02, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

phi= 0.8          
z1=rep(0,500)                                 
z1[1]=0
for (i in 3:500) z1[i]=phi*z1[i-1]+w[i]   
plot(ts(z1), main="Proceso autoregresivo orden 1", xlab="tiempo", ylab="Serie")  

```



---

## Procesos autoregresivos 

- Proceso autoregresivo de orden 1 $x_t = 0.8x_{t-1} + w_t$

```{r aracf02, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6}
ggAcf(z1) + labs(title = "Proceso autoregresivo orden 1")
```


---

## Procesos autoregresivos 

- Proceso autoregresivo de orden 2 $x_t = x_{t-1} - 0.9x_{t-2} + w_t$

```{r arpacf02, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6}
ggPacf(z1) + labs(title = "PACF Proceso autoregresivo orden 1")
```


---


## Random walk (Paseo aleatorio)

- Paseo aleatorio $x_t = x_{t-1} + w_t$

- Paseo aleatorio con deriva $x_t = \delta+ x_{t-1} + w_t$

```{r rw01, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

set.seed(123)
TT <- 500
zz <- xx <- ww <- rnorm(n = TT, mean = 0, sd = 1)
for (t in 2:TT) {
  xx[t] <- xx[t - 1] + ww[t]
}
delta=0.2
for (t in 2:TT) {
  zz[t] <- delta + zz[t - 1] + ww[t]
}
plot(xx %>% ts, type="l", ylim=c(-20,100) ,ylab = "Series", xlab="Tiempo", main="Paseo Aleatorio")
lines(zz %>% ts)
lines(delta*(0:500), lty = "dashed", col=2)


```

---

## Estacionariedad


- Si $\{y_t\}$ es una serie temporal estacionaria, entonces para todo $s$, la distribución de $(y_t,\dots,y_{t+s})$ no depende de $t$.

- Las transformaciones ayudan a **estabilizar la varianza**.

- Para la modelización ARIMA, también necesitamos **estabilizar la media**.

---

## No estacionariedad en la media

Identificación de series no estacionarias


* Gráfico de la serie temporal.

* La ACF de los datos estacionarios cae a cero con relativa rapidez

* La ACF de los datos no estacionarios disminuye lentamente.

* Para los datos no estacionarios, el valor de $r_1$ es a menudo
     grande y positivo.
     
---

## Ejemplo: Indice Dow-Jones

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6, echo=FALSE}
autoplot(dj) + ylab("Indice Dow-Jones") + xlab("Día")
```


---

## Ejemplo: Indice Dow-Jones

```{r , warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6, echo=FALSE}
ggAcf(dj)
```

---

## Ejemplo: Indice Dow-Jones

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6, echo=FALSE}
autoplot(diff(dj)) + ylab("Variación del Indice Dow-Jones") + xlab("Día")
```

---

## Ejemplo: Indice Dow-Jones

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6, echo=FALSE}
ggAcf(diff(dj))
```

---

## Diferenciación

* La diferenciación ayuda a **estabilizar la media**.

* La serie diferenciada es el *cambio* entre cada observación
en la serie original: ${y'_t = y_t - y_{t-1}}$.

* La serie diferenciada sólo tendrá valores $T-1$ ya que no es posible calcular una diferencia $y_1'$ para la primera observación.

---

## Diferenciación de segundo orden

En ocasiones, los datos diferenciados no parecen estacionarios y puede ser necesario
puede ser necesario diferenciar los datos una segunda vez:

$$
\begin{aligned}
y''_{t}  &=  y'_{t}  - y'_{t - 1} \\
&= (y_t - y_{t-1}) - (y_{t-1}-y_{t-2})\\
&= y_t - 2y_{t-1} +y_{t-2}.
\end{aligned}
$$

* $y_t''$ tendrá $T-2$ valores.

* En la práctica, casi nunca es necesario ir más allá de las diferencias de segundo orden


---

## Diferencia estacional

Una diferencia estacional es la diferencia entre una observación y la observación correspondiente del año anterior.
$$y'_t = y_t - y_{t-m}$$
donde $m=$ número de estaciones

* Para datos mensuales $m=12$.

* Para datos trimestrales $m=4$.

---

## Producción de electricidad en Australia

Datos originales

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6, echo=FALSE}
usmelec %>% autoplot()
```

---

## Producción de electricidad en Australia

Aplicamos logaritmo

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6, echo=FALSE}
usmelec %>% log() %>% autoplot()
```

---

## Producción de electricidad en Australia

Aplicamos logaritmo + diferencia estacional (12 meses)

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6, echo=FALSE}
usmelec %>% log() %>% diff(lag=12) %>%
  autoplot()
```

---

## Producción de electricidad en Australia

Aplicamos logaritmo + diferencia estacional (12 meses) + diferencia regular (1)

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6, echo=FALSE}
usmelec %>% log() %>% diff(lag=12) %>%
  diff(lag=1) %>% autoplot()
```

---

## Producción de electricidad en Australia

* Las series diferenciadas estacionalmente están más cerca de ser estacionarias.

* La no estacionariedad restante puede ser eliminada con una primera diferencia adicional.

Si $y'_t = y_t - y_{t-12}$ denota una serie diferenciada estacionalmente, entonces la serie diferenciada dos veces 

$$
\begin{aligned}
y^*_t &= y'_t - y'_{t-1} \\
      &= (y_t - y_{t-12}) - (y_{t-1} - y_{t-13}) \\
      &= y_t - y_{t-1} - y_{t-12} + y_{t-13}\: .
\end{aligned}
$$

---

## Diferencias estacionales

Cuando se aplican tanto las diferencias estacionales como las primera diferencia

* no hay ninguna diferencia en cuál de las dos se haga primero: el resultado será el mismo.

* Si la estacionalidad es fuerte, recomendamos que se haga primero la diferenciación estacional porque a veces la serie resultante será estacionaria y no habrá necesidad de más primera diferencia.

Es importante que si se utiliza la diferenciación, las diferencias sean interpretables.


---

## Interpretación de las diferencias

* Las primeras diferencias son el cambio entre **una observación y la anterior**;

* las diferencias estacionales son el cambio entre **un año y el anterior**.


Pero si se toman las diferencias del tercer retardo para los datos anuales, por ejemplo, se obtiene un modelo que no puede interpretarse de forma razonable.

---

## Operador de diferencias

El operador de diferencias se denota con la $\Delta$

### Diferencia regular (un período)

$$
\begin{aligned}
\Delta y_t  &= y_t - y_{t-1} \\
\Delta^2 y_t &= \Delta (y_t - y_{t-1}) &=  (y_t - y_{t-1}) - (y_{t-1}-y_{t-2}) &= y_t - 2y_{t-1} +y_{t-2}\\
\end{aligned}
$$

### Diferencia estacional

$$\Delta_s y_t= y_t-y_{t-s}$$

---

## Operador de retardos

$$
\begin{aligned}
Ly_t &=y_{t-1}\\
L^2y_t &=y_{t-2}\\
\vdots & \vdots \\
L^sy_t &=y_{t-s}\\
\end{aligned}
$$

En palabras, $L$, operando en $y_{t}$, tiene el efecto de **desplazar los datos hacia atrás un período**.

Dos aplicaciones de $L$ a $y_{t}$ **desplazan los datos hacia atrás dos periodos** y así sucesivamente

---

## Operador de retardos

En el caso de los datos mensuales, si deseamos concentrar la atención a "el mismo mes del año pasado'', entonces se utiliza $L^{12}$
y la notación es notación es $L^{12}y_{t}$ = $y_{t-12}$.

El operador de retardo hacia atrás es conveniente para describir el proceso de **diferenciación**. Una primera diferencia se puede escribir como

$$y'_{t} = y_{t} - y_{t-1} = y_t - Ly_{t} = (1 - L)y_{t}$$

Tenga en cuenta que una primera diferencia se representa por $(1 - L)$.

Del mismo modo, si hay que calcular diferencias de segundo orden (es decir, primeras diferencias de primeras diferencias), entonces:
$$y''_{t} = y_{t} - 2y_{t - 1}  + y_{t - 2} = (1 - L)^{2} y_{t}$$

---

## Operador de retardos

* La diferencia de segundo orden se denota $(1- L)^{2}$.

* **diferencia de segundo orden** no es lo mismo que **segunda diferencia**, que se denotaría $L^{2}$;

* En general, una diferencia de orden $d$ se puede escribir como

$$(1 - L)^{d} y_{t}.$$

* Una diferencia estacional seguida de una primera diferencia puede escribirse como

$$ (1-L)(1-L^m)y_t$$

---

## Operador de retardos

La notación en operador de retardos es conveniente porque los términos pueden multiplicarse juntos para ver el efecto combinado.

$$
\begin{aligned}
(1-B)(1-B^m)y_t &= (1 - B - B^m + B^{m+1})y_t \\
&= y_t-y_{t-1}-y_{t-m}+y_{t-m-1}.
\end{aligned}
$$

Para datos mensuales, $m=12$ y obtenemos el mismo resultado que antes.
 
---

## Operador de retardos en polinomios

$$
\begin{aligned}
y_t&=\phi_1 y_{t-1}+\phi_2 y_{t-2}+\ldots+\phi_p y_{t-p}+\epsilon_t\\
y_t-\phi_1 y_{t-1}-\phi_2 y_{t-2}-\ldots-\phi_p y_{t-p}&=\epsilon_t\\
y_t-\phi_1 Ly_t-\phi_2 L^2y_t-\ldots-\phi_p L^py_t&=\epsilon_t\\
(1-\phi_1 L-\phi_2 L^2-\ldots-\phi_p L^p)y_t&=\epsilon_t\\
\phi(L)y_t&=\epsilon_t
\end{aligned}
$$

Esta notación se empleará de manera extensa a lo largo de la asignatura

---

## Modelos autorregresivos

Modelos autorregresivos (AR):
$$y_{t} = c + \phi_{1}y_{t - 1}  + \phi_{2}y_{t - 2} + \cdots + \phi_{p}y_{t - p}  + \varepsilon_{t},$$
donde $\varepsilon_t$ es ruido blanco.  Se trata de una regresión múltiple con **valores retardados** de $y_t$ como predictores.


```{r arp, echo=FALSE, fig.height=5.5, fig.width=12}
set.seed(1)
p1 <- autoplot(10 + arima.sim(list(ar = -0.8), n = 100)) +
  ylab("") + ggtitle("AR(1)")
p2 <- autoplot(20 + arima.sim(list(ar = c(1.3, -0.7)), n = 100)) +
  ylab("") + ggtitle("AR(2)")
gridExtra::grid.arrange(p1,p2,nrow=1)
```

---

## Modelos autorregresivos

### Modelo AR(1)

$$y_{t}    =   2 -0.8 y_{t - 1}  +  \varepsilon_{t}$$

$$\varepsilon_t\sim N(0,1),\quad T=100$$

```{r, echo=FALSE, fig.height=5, fig.width=12}
p1
```


---

## Modelos autorregresivos

### Modelo AR(1)


$$y_{t}    =   c + \phi_1 y_{t - 1}  +  \varepsilon_{t}$$



* Cuando $\phi_1=0$, $y_t$ es **equivalente a un ruido blanco**
* Cuando $\phi_1=1$ y $c=0$, $y_t$ es **equivalente a un ruido blanco**
* Cuando $\phi_1=1$ y $c\ne0$, $y_t$ es **equivalente a un ruido blanco con deriva**
* Cuando $\phi_1<0$, $y_t$ tiende a **oscila entre valores positivos y negativos**.

---

## Modelos autorregresivos

### Modelo AR(2)

$$y_t = 8 + 1.3y_{t-1} - 0.7 y_{t-2} + \varepsilon_t$$

$$\varepsilon_t\sim N(0,1), \qquad T=100$$

```{r, fig.height=5, fig.width=12}
p2
```

---

## Condiciones de estacionariedad

Normalmente restringimos los modelos autorregresivos a datos estacionarios, por lo que se requieren algunas restricciones sobre los valores de los parámetros.

### Condición general de estacionariedad

Las raíces complejas de $1-\phi_1 z - \phi_2 z^2 - \dots - \phi_pz^p$ se encuentran fuera del círculo unitario en el plano complejo.


* Para $p=1$: $-1<\phi_1<1$.
* Para $p=2$:
  $-1<\phi_2<1\qquad \phi_2+\phi_1 < 1 \qquad \phi_2 -\phi_1 < 1$.
* Condiciones más complicadas se mantienen para $p\ge3$.

---

## Modelos de medias móviles (MA)

$$y_{t}  =  c +  \varepsilon_t + \theta_{1}\varepsilon_{t - 1}  +  \theta_{2}\varepsilon_{t - 2}  +  \cdots  + \theta_{q}\varepsilon_{t - q},$$
donde $\varepsilon_t$ es ruido blanco.

Se trata de una regresión múltiple con **errores pasados como predictores.** 

No confunda esto con el suavizado de la media móvil.


```{r maq, fig.height=5, fig.width=12, echo=FALSE}
set.seed(2)
p1 <- autoplot(20 + arima.sim(list(ma = 0.8), n = 100)) +
  ylab("") + ggtitle("MA(1)")
p2 <- autoplot(arima.sim(list(ma = c(-1, +0.8)), n = 100)) +
  ylab("") + ggtitle("MA(2)")
gridExtra::grid.arrange(p1,p2,nrow=1)
```
 
---
 
## Modelo MA(1) 


$$y_t = 20 + \varepsilon_t + 0.8 \varepsilon_{t-1}$$

$$\varepsilon_t\sim N(0,1),\quad T=100.$$

```{r, fig.height=5, fig.width=12}
p1
```

---

## Modelo MA(2)


$$y_t = \varepsilon_t -\varepsilon_{t-1} + 0.8 \varepsilon_{t-2}$$

$$\varepsilon_t\sim N(0,1),\quad T=100$$

```{r, fig.height=5, fig.width=12}
p2
```


---


## Modelo MA(<span>&#8734;</span>) 

Es posible escribir cualquier proceso estacionario AR(*p*) como un proceso MA(<span>&#8734;</span>).

### Ejemplo: AR(1)

$$
\begin{align*}
y_t &= \phi_1y_{t-1} + \varepsilon_t\\
&= \phi_1(\phi_1y_{t-2} + \varepsilon_{t-1}) + \varepsilon_t\\
&= \phi_1^2y_{t-2} + \phi_1 \varepsilon_{t-1} + \varepsilon_t\\
&= \phi_1^3y_{t-3} + \phi_1^2\varepsilon_{t-2} + \phi_1 \varepsilon_{t-1} + \varepsilon_t\\
&\dots
\end{align*}
$$

Dado que $-1 < \phi_1 < 1$:

$$y_t = \varepsilon_t + \phi_1 \varepsilon_{t-1} + \phi_1^2 \varepsilon_{t-2} + \phi_1^3 \varepsilon_{t-3} + \cdots$$

---

## Invertibilidad

- Cualquier proceso MA(*q*) puede ser escrito como un proceso AR(<span>&#8734;</span>) si imponemos algunas restricciones en los parámetros MA.

- Entonces el modelo MA se llama "invertible".

- Los modelos invertibles tienen algunas propiedades matemáticas que facilitan su uso en la práctica.

- La invertibilidad de un modelo ARIMA es equivalente a la previsibilidad de un modelo HTA.


---

## Invertibilidad

### Condición general de invertibilidad

Las raíces complejas de $1+\theta_1 z + \theta_2 z^2 + \dots + \theta_qz^q$ están fuera del círculo unitario en el plano complejo.


* Para $q=1$: $-1<\theta_1<1$.

* Para $q=2$: $-1<\theta_2<1\qquad \theta_2+\theta_1 >-1 \qquad \theta_1 -\theta_2 < 1$.

* Condiciones más complicadas se mantienen para $q\ge3$.




