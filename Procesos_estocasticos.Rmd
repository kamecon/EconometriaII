---
title: "Procesos Estocásticos y Series Temporales"
subtitle: "Econometría II"
author: "Kamal Romero"
date: "(actualizado: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
```

```{r librerias, echo = FALSE, warning=FALSE, message=FALSE}
if (! ('pacman' %in% installed.packages())) install.packages('pacman')
pacman::p_load(tidyverse, kableExtra, quantmod, fredr, patchwork, fpp2)
```

## Tipo de Cambio Nominal

- El comportamiento del tipo de cambio es lejos de ser sistemático y predecible

```{r er0librerias, echo = FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Cargamos los datos
load("data/euro_dolar.RData")
load("data/euro_libra.RData")
load("data/euro_yen.RData")

```

```{r er1, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

plot(index(euro_dolar),as.numeric(euro_dolar[,6]),typ='l',xlab='',ylab='EUR/USD', main = "Tipo de Cambio Euro/Dólar")


```

---

## Tipo de Cambio Nominal

- El comportamiento del tipo de cambio es lejos de ser sistemático y predecible

```{r er2, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

plot(index(euro_libra),as.numeric(euro_libra[,6]),typ='l',xlab='',ylab='EUR/GBP', main = "Tipo de Cambio Euro/Libra")


```

---

## Tipo de Cambio Nominal

- El comportamiento del tipo de cambio es lejos de ser sistemático y predecible

```{r er3, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

plot(index(euro_yen),as.numeric(euro_yen[,6]),typ='l',xlab='',ylab='EUR/JPY', main = "Tipo de Cambio Euro/Yen")


```

---

## Tipo de Cambio Nominal

- Aunque sus variaciones presentan un comportamiento más predecible

```{r er4, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

plot(index(euro_dolar),as.numeric(euro_dolar[,7]),typ='l',xlab='',ylab='EUR/USD', main = "Variación del Tipo de Cambio Euro/Dólar")

```

---

## Tipo de Cambio Nominal

- Aunque sus variaciones presentan un comportamiento más predecible

```{r er5, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

plot(index(euro_libra),as.numeric(euro_libra[,7]),typ='l',xlab='',ylab='EUR/GBP', main = "Variación del Tipo de Cambio Euro/Libra")

```

---

## Tipo de Cambio Nominal

- Aunque sus variaciones presentan un comportamiento más predecible

```{r er6, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

plot(index(euro_yen),as.numeric(euro_yen[,7]),typ='l',xlab='',ylab='EUR/USD', main = "Variación del Tipo de Cambio Euro/Yen")

```

---

## Proceso estocástico y serie temporal

- Una serie temporal es una colección de observaciones de una variable tomadas de forma secuencial y ordenada en el tiempo (instantes de tiempo equiespaciados). Las series pueden tener una periodicidad anual, semestral, trimestral, mensual, semanal, diaria etc., según los periodos de tiempo en los que están recogidos los datos que la componen.

- Un proceso estocástico es una secuencia de variables aleatorias, ordenadas y equidistantes cronológicamente referidas a una característica observable en diferentes momentos.

- Por ello, una serie temporal es una **realización particular de una muestra procedente de un proceso estocástico.**


---

## Proceso estocástico y serie temporal

- Una serie temporal es una realización particular de un proceso estocástico

```{r proceso01, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

# Parámetros
phi= -0.7048                                 
y=rep(0,100)
cons=2.1                                     
y[1]=4.1

#Creamos un data frame vacio que luego rellenamos con distintas realizaciones de series temporales
datos <- matrix(0, nrow = 100, ncol = 10) %>% data.frame()

for (i in 1:10) {
  datos[1,i]=4.1
  e=rnorm(n=100,0,0.7) 
  for (j in 2:100) datos[j,i]=cons+phi*datos[j-1,i]+e[j]   
}

plot(datos[,1] %>% ts(), col=2, lwd=3, main="Proceso Estocástico", xlab="Tiempo", ylab="Serie")

```

---

## Proceso estocástico y serie temporal

- Una serie temporal es una realización particular de un proceso estocástico

```{r proceso02, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

plot(datos[,1] %>% ts(), col=2, lwd=3, main="Proceso Estocástico", xlab="Tiempo", ylab="Serie")
lines(datos[,2] %>% ts(), col=3)

```


---

## Proceso estocástico y serie temporal

- Una serie temporal es una realización particular de un proceso estocástico

```{r proceso03, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

plot(datos[,1] %>% ts(), col=2, lwd=3, main="Proceso Estocástico", xlab="Tiempo", ylab="Serie")
lines(datos[,2] %>% ts(), col=3)
lines(datos[,3] %>% ts(), col=4)

```


---

## Proceso estocástico y serie temporal

- Una serie temporal es una realización particular de un proceso estocástico

```{r proceso04, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

plot(datos[,1] %>% ts(), col=2, lwd=5, main="Proceso Estocástico", xlab="Tiempo", ylab="Serie")
for (i in 2:10) {
  lines(datos[,i] %>% ts(), col=i+1)
}

```


---

## Proceso estocástico y serie temporal

<img src="proceso01.jpg" width="700">


---

## Proceso estocástico y serie temporal
 
### Distribución de un Proceso Estocástico

- En analogía con las variables aleatorias (resultado de lanzar una moneda) queremos introducir caracteristicas no aletorias de los procesos estocásticos tales como su distribución, su esperanza, varianza, etc, y describir su **estructura de dependencia**.

- Esta es una tarea mucho más complicada que en el caso de vectores de variables aleatorias (lanzar una moneda *n* veces).

- De hecho un proceso estocástico no-trivial con un conjunto índice $T$ es un objeto de dimensión infinita en el sentido de que se puede entender como una colección infinita de variables aleatorias.

- En este curso intentaremos algo mucho mas simple.

---


## Proceso estocástico y serie temporal


  - Elaborar un **modelo estadístico** para una muestra procedente de un proceso estocástico $Y_t$ a partir de una única realización particular (una serie temporal $y_t$).
  
  - Utilizar el modelo elaborado para **prever** los futuros valores de proceso $Y_{(N+l)}$.
  
  - **Contrastar** alguna teoría sobre la característica o variable a la que se refiere la serie considerada.
  

---


## Proceso estocástico y serie temporal

### ¿Cómo lo vamos a hacer?

Al igual que en la Econometría básica trabajábamos con dos los supuestos de *i.i.d.* (independiente e idénticamente distribuido), en la Econometría de Series Temporales nos hace faltan dos supuestos equivalentes:

- <span style="color:red">**Estacionariedad**</span> (substituye al supuesto de identicamente distribuido)

- <span style="color:red">**Ergodicidad**</span> (substituye al supuesto de independencia)


  
---


## Proceso estocástico y serie temporal

### ¿Cómo lo vamos a hacer?

- Para conseguir estos objetivos se va a seleccionar un modelo para la muestra dentro de una clase general de modelos denominados ARIMA, que implique cierta propiedades teóricas para el proceso estocástico $Y_t$ del que procede la muestra y que resulten compatibles con las propiedades muestrales observadas en la serie temporal $y_t$.

- El problema para hacer esto es que solo tenemos una única realización particular del proceso estocástico. Para solucionar esto necesitamos que se cumpla la hipótesis de estacionariedad mencionado antes.
  
  
---

## Estacionariedad: Definición


Si $\{y_t\}$ es una serie temporal estacionaria, entonces para todo $s$, la distribución de $(y_t,\dots,y_{t+s})$ no depende de $t$.


Una **serie estacionaria** es:

*  aproximadamente horizontal
*  varianza constante
*  no hay patrones predecibles a largo plazo 
---

## ¿Estacionario?

```{r fig.width=12, fig.height=7.5}
autoplot(dj) + ylab("Dow Jones Index") + xlab("Día")
```

---

## ¿Estacionario?

```{r fig.width=12, fig.height=7.5}
autoplot(diff(dj)) + ylab("Cambio en el Dow Jones Index") + xlab("Día")
```

---


## ¿Estacionario?
```{r fig.width=12, fig.height=7.5}
autoplot(eggs) + xlab("Año") + ylab("$") +
  ggtitle("Precio de una docena de huevos en dólares de 1993")
```

---


## ¿Estacionario?

```{r fig.width=12, fig.height=7.5}
autoplot(window(ausbeer, start=1992)) + xlab("Año") + ylab("megalitros") +
  ggtitle("Producción trimestral de cerveza en Australia")
```


---

## Estacionariedad: Definición

Vamos a trabajar este concepto desde su definición más simple a una con mayor complejidad

Si $\{y_t\}$ es una serie temporal estacionaria, entonces para todo $s$, la distribución de $(y_t,\dots,y_{t+s})$ no depende de $t$.


Una **serie estacionaria** es:

*  aproximadamente horizontal
*  varianza constante
*  no hay patrones predecibles a largo plazo 

---

## Estacionariedad: Definición (más formal)

Un proceso se dice que es **estacionario debil** de orden *n* si todos sus momentos conjuntos de orden *n* existen y son invariantes en el tiempo.

Procesos Estacionarios en Covarianzas (de 2º orden):

- Esperanza constante

- Varianza constante

- La función de covarianzas depende solo de la diferencia temporal entre las variables


---

## Estacionariedad: Definición (aún más formal)

- Un proceso estocástico $(Y_t)$ es **estrictamente estacionario** si y sólo si para cualesquiera $n \geq 1$ momentos $t_1<t_2< \ldots t_n$ de su historia, la distribución de probabilidad conjunta de $[Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n}]'$ coincide con la de $[Y_{t_1+h}, Y_{t_2+h}, \ldots, Y_{t_n+h}]'$ para cualquier número entero $h=\pm 1, \pm 2, \ldots$ (distinto de cero).

---

## Estacionariedad: Definición (aún más formal)

- Un proceso estocástico $(Y_t)$ con $E(Y_t) < \infty$ para todo $t=\pm 1, \pm 2, \ldots$ es **estacionario en media** o **débilmente estacionario de primer orden** si y solo si $E(Y_t)$ es constante (no depende de $t$) para todo $t=\pm 1, \pm 2, \ldots$.

- Un proceso estocástico $(Y_t)$ con $E(Y^2_t) < \infty$ para todo $t= \pm 1, \pm 2, \ldots$ es **estacionario en autocovarianza** o **débilmente estacionario de segundo orden** si y solo si:

   - $E(Y_t)$ y $Var(Y_t)$ son constantes (no depende de $t$) para todo $t= \pm 1, \pm 2, \ldots$.

   - $Cov(Y_t, Y_{t+k})$ depende a lo sumo de $k$ pero no de $t$ para todo $t=\pm 1, \pm 2, \ldots$.

---

## Estacionariedad: Definición (aún más formal)

- Un proceso estocástico $(Y_t)$ es **normal** o **gaussiano** cuando para cualesquiera $n \geq 1$ momentos $t_1<t_2< \ldots t_n$ de su historia, la distribución de probabilidad conjunta de $[Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n}]'$ es una distribución normal $n-$variante.

> <span style="color:red"> Estacionariedad en autocovarianza + Normalidad $\Rightarrow$ Estacionariedad Estricta </span>

---


## Propiedades  de la hipótesis de estacionariedad

   - <span style="color:red">**Estacionariedad**</span>: cierto estado de equilibrio estadístico que caracteriza la evolución temporal de un proceso estocástico que ha genera una serie temporal.
   
   -  <span style="color:red">**Media**</span>: valor constante en el tiempo que mide el nivel alrededor del cual evoluciona un proceso estocástico estacionario.
   
   -  <span style="color:red">**Varianza**</span>: valor constante en el tiempo que mide la dispersión o la variabilidad de la evolución temporal de un proceso estocástico estacionario alrededor de su media.
   
   -  <span style="color:red">**Autocorrelaciones**</span>: valores constantes en el tiempo que miden el grado de asociación lineal entre cada par de componentes de un proceso estocástico estacionario separados por diferentes intervalos temporales o retardos.

---

## Autocorrelación

**Covarianza** y **correlación**: miden el grado de **relación lineal** entre dos variables $y$ y $X$.

**Autocovarianza** y **autocorrelación**: miden la relación lineal entre los **valores retardados** de una serie temporal $y$.

Medimos la relación entre:

  * $y_{t}$ y $y_{t-1}$
  * $y_{t}$ y $y_{t-2}$
  * $y_{t}$ y $y_{t-3}$
  * etc.

---

## Autocorrelación

Denotamos la autocovarianza muestral con retardo $k$ por $c_k$ y la autocorrelación muestral con retardo $k$ por $r_k$.  Entonces definimos

$$c_k = \frac{1}{T}\sum_{t=k+1}^T (y_t-\bar{y})(y_{t-k}-\bar{y})$$

y

$r_{k} = c_k/c_0$


  * $r_1$ indica cómo se relacionan los valores sucesivos de $y$
  * $r_2$ indica cómo se relacionan los valores de $y$ con dos períodos de diferencia
  * $r_k$ es *casi* lo mismo que la correlación muestral entre $y_t$ y $y_{t-k}$.

---

## Autocovarianzas teóricas

- La **autocovarianza de orden $k$** $(k > 0)$ de un proceso $(Y_t)$ estacionario se representa con el símbolo $\gamma_k$ y se define como $\gamma_k \equiv Cov(Y_t, Y_{t+k})$.

$$\begin{array}{|cc|}
\hline
Media  & \mu_Y \equiv E(Yt) \\
Varianza:  & \sigma^2_Y \equiv Var(Y_t) \equiv E[(Y_t - \mu_Y)^2]  \\
Autocov. orden\; k:  & \gamma_k \equiv Cov(Y_t, Y_{t+k}) \equiv E[(Y_t - \mu_Y)(Y_{t+k} - \mu_Y)](k=1,\ldots) \\
\hline
\end{array}$$


- La autocovarianza de orden $k$ de $(Y_t)$ es la covarianza entre cualquier par de componentes de $(Y_t)$ separados entre sí por un intervalo temporal o **retardo** $(k > 0)$ dado.

- La autocovarianza de orden cero como $\gamma_k \equiv Cov(Y_t, Y_t) \equiv Var(Y_t)$, que es la varianza del proceso estacionario $(Y_t)$   

- La secuencia $(\gamma_k:k =0, 1, 2, \ldots)$ se denomina **la función de autocovarianza** del proceso estacionario $(Y_t)$.


---

## Autocorrelaciones simples teóricas

- La **autocorrelación simple de orden $k$** $(k>0)$ de un proceso $(Y_t)$ estacionario se representa con el símbolo $\rho_k$ y se define como:

$$\rho_k \equiv \frac{Cov(Y_t, Y_{t+k})}{Var(Y_t)^{1/2}Var(Y_{t+k})^{1/2}} \equiv \frac{\gamma_k}{\gamma_0}$$


$$\begin{array}{|cc|}
\hline
Autocorrelacion\; simple\; de\; orden \; k:  & \displaystyle\frac{\gamma_k}{\gamma_0} (k=1,2,\ldots); \; \rho_0=1 \\
\hline
\end{array}$$

- Considerada como una función de $k$, $(\rho_k: k = 1, 2, \ldots)$ se denomina **la función de autocorrelación simple (ACF)** del proceso estacionario $(Y_t)$. Dado que cada $\rho_k$ es un coeficiente de correlación, suele decirse que la ACF de $(Y_t)$ represente la **duración** y la **intensidad** de la **memoria** del proceso $(Y_t)$.

---

## PACF teórica


- La **autocorrelación parcial de orden $k$** de un proceso $(Y_t)$ estacionario se define como el parámetro $\phi_{kk}$ en la regresión:

$$\tilde{Y_t} = \phi_{k0} + \phi_{k1}\tilde{Y}_{t-1} + \phi_{k2}\tilde{Y}_{t-2} + \ldots + \phi_{kk}\tilde{Y}_{t-k} + U_t$$

- Donde $\tilde{Y}_{t-i} \equiv Y_{t-i} - \mu_Y \; (i=0,1,\ldots,k)$ y $U_t$ es independiente de $Y_{t-i}$ para todo $i \geq 1$

- Además, $\triangle Y_t = \phi_{kk} \times \triangle Y_{t-k}$ cuando $\triangle Y_{t-1} = \triangle Y_{t-2} = \ldots = \triangle Y_{t-k+1} = \triangle U_{t} =0$.

- $\phi_{kk}$ representa el **efecto parcial** o **ceteris paribus** de $\triangle Y_{t-k}$ sobre $\triangle Y_{t}$.

- $\phi_{kk}$ es una medida del grado de asociación lineal entre dos componentes cualesquiera de $(Y_t)$ separados entre sí por el retardo $k$ dado, que **no** es debida a la posible correlación entre los componentes de $(Y_t)$ que se encuentran entre ambos.


---

## PACF teórica

- A partir de la regresión anterior, puede comprobarse que $\phi_{kk}=|A_k|/|B_k|$, donde

$$A_k \equiv \left(
               \begin{array}{cccccc}
                 1 & \rho_1 & \rho_2 & \cdots & \rho_{k-2} & \rho_1 \\
                 \rho_1 & 1 & \rho_1 & \cdots & \rho_{k-3} & \rho_2 \\
                 \rho_2 & \rho_1 & 1 & \cdots & \rho_{k-4} & \rho_3 \\
                 \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
                 \rho_{k-1} & \rho_{k-2} & \rho_{k-3} & \cdots & \rho_{1} & \rho_k \\
               \end{array}
             \right)$$
             
$$B_k \equiv \left(
               \begin{array}{cccccc}
                 1 & \rho_1 & \rho_2 & \cdots & \rho_{k-2} & \rho_{k-1} \\
                 \rho_1 & 1 & \rho_1 & \cdots & \rho_{k-3} & \rho_{k-2} \\
                 \rho_2 & \rho_1 & 1 & \cdots & \rho_{k-4} & \rho_{k-3} \\
                 \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
                 \rho_{k-1} & \rho_{k-2} & \rho_{k-3} & \cdots & \rho_{1} & 1 \\
               \end{array}\right)$$


---

## PACF teórica

- En particular, para $k=1,2,3$ resulta que:

$$\phi_{11} = \rho_1 , \;\; \phi_{22} = \frac{\left|\begin{array}{cc}
                                             1 & \rho_1 \\
                                             \rho_1 & \rho_2 \\
                                           \end{array}\right|}{\left|\begin{array}{cc}
                                             1 & \rho_1 \\
                                             \rho_1 & 1 \\
                                           \end{array}\right|} = \frac{\rho_2-\rho^2_1}{1-\rho_1^2}$$

---

 ## PACF teórica

- Lo que muestra la anterior expresión es que cada coeficiente de autocorrelación parcial $\phi_{kk}$ de un proceso estrictamente estacionario es una función concreta de los coeficientes de autocorrelación simple $\rho_1, \rho_2, \ldots, \rho_k$ $(k=1,2,3, \ldots)$.

- Considerada como una función del retardo $k$, la secuencia $(\phi_{kk}:k=1,2,\ldots)$ se denomina **la función de autocorrelación parcial (PACF)**

$$\begin{array}{|cc|}
\hline
Autocorrelación\; parcial\; de\; orden\; k: & \phi_{kk}= \frac{|A_k|}{|B_k|} (k=1,2,\ldots); \phi_{11} = \rho_1 \\
\hline
\end{array}$$

---

## Problema de estimación


- Si una muestra $Y \equiv [Y_1, Y_2, \ldots, Y_N]'$ **no procede de un proceso estacionario**, entonces su vector de medias y su matriz de varianzas-covarianzas son:
 
$${\mu} \equiv E(Y) = \left(
                        \begin{array}{c}
                          \mu_1 \\
                          \mu_2 \\
                          \vdots \\
                          \mu_N \\
                        \end{array}
                      \right)$$
 
 
$$\Sigma \equiv Var(Y) = \left(
                          \begin{array}{cccc}
                             \sigma^2_1 & \sigma_{12} & \cdots & \sigma_{1N} \\
                             \sigma_{21} & \sigma^2_2 & \cdots & \sigma_{2N} \\
                             \vdots & \vdots & \ddots & \vdots \\
                             \sigma_{N1} & \sigma_{N2} & \cdots & \sigma^2_N \\
                           \end{array}\right)$$
                       
                       
- Las anteriores expresiones contienen $N+N(N+1)/2$ parámetros distintos. No pueden estimarse con precisión este número de parámetros utilizando tan solo una única serie temporal $y \equiv (y_1, y_2, \ldots, y_N)'$ de $N$ observaciones. 

- Si utilizamos una serie de tiempo que procede de un proceso estacionario, reducimos el número de parámetros a estimar

---

## Problema de estimación


- Si una muestra $Y \equiv [Y_1, Y_2, \ldots, Y_N]'$ procede de un proceso estacionario, entonces su vector de medias y su matriz de varianzas-covarianzas son:
 
$${\mu} \equiv E(Y) = \left(
                        \begin{array}{c}
                          \mu_Y \\
                          \mu_Y \\
                          \vdots \\
                          \mu_Y \\
                        \end{array}
                      \right)$$
 
$$\Sigma \equiv Var(Y) = \left(
                          \begin{array}{cccc}
                             \gamma_0 & \gamma_1 & \cdots & \gamma_{N-1} \\
                             \gamma_1 & \gamma_0 & \cdots & \gamma_{N-2} \\
                             \vdots & \vdots & \ddots & \vdots \\
                             \gamma_{N-1} & \gamma_{N-2} & \cdots & \gamma_0 \\
                           \end{array}\right)$$

                       
 
---

## Problema de estimación

$$\Sigma =\sigma^2_Y\left(
                \begin{array}{cccc}                                                                                                        1 & \rho_1 & \cdots & \rho_{N-1} \\
                  \rho_1 & 1 & \cdots & \rho_{N-2} \\
                  \vdots & \vdots & \ddots & \vdots \\
                  \rho_{N-1} & \rho_{N-2} & \cdots & 1 \\
                \end{array}\right)$$
                                                                                              
- Las anteriores expresiones contienen $1+N$ parámetros distintos. A pesar que la hipótesis de estacionariedad, no pueden estimarse con precisión $1+N$ parámetros utilizando tan solo una única serie temporal $y \equiv (y_1, y_2, \ldots, y_N)'$ de $N$ observaciones.

- La solución a este problema pasa por expresar la media y la función de autocovarianza de $Y_t$ en términos de un número reducido de parámetros, a través del uso de un **modelo ARMA** para $(Y_t)$.


---

## ACF y PACF muestrales


- En la práctica es imposible estimar a partir de una serie temporal de $N$ observaciones, la media, la varianza, la ACF y PACF del proceso estacionario del que supuestamente procede dicha serie.

- Sin embargo, sí pueden estimarse con cierta precisión las $K$ primeras autocorrelaciones simples y parciales del proceso estocástico a través de una serie temporal de $N$ observaciones, siempre que $K$ sea un número bastante más reducido que $N$.

- La **media muestral** y la **varianza muestral** de una muestra asociada con una serie temporal son:

$$\widehat{\mu}_Y \equiv \frac{1}{N}\sum_{t=1}^N Y_t\;\;\; \text{y} \;\;\;\widehat{\sigma}_Y^2 \equiv \frac{1}{N} \sum_{t=1}^N (Y_t-\widehat{\mu}_Y)^2$$



---

## ACF muestral

- La **correlación simple muestral de orden $k$** $(k>0)$ de una muestra asociada a una serie temporal es:
$$\widehat{\rho}_k \equiv \frac{\widehat{\gamma}_k}{\widehat{\gamma}_0},(k=1,2,\ldots)$$

donde:

$$\begin{array}{rcl}
\widehat{\gamma}_k & \equiv & \displaystyle\frac{1}{N} \displaystyle\sum_{t=1}^{N-k} (Y_t-\widehat{\mu}_Y)(Y_{t+k}-\widehat{\mu}_Y)\\
                   & \equiv & \displaystyle\frac{1}{N} \displaystyle\sum_{t=k+1}^N (Y_{t-k}-\widehat{\mu}_Y)(Y_t-\widehat{\mu}_Y) \; (k=0,1,2,\ldots)\\
                   \end{array}$$

---

## ACF muestral

- Los estimadores $\widehat{\gamma}_k$ y $\widehat{\rho}_k$ proporcionan **estimaciones** numéricas $c_k$ y $r_k$, respectivamente,

$$c_k \equiv \frac{1}{N} \sum_{t=1}^{N-k} (y_t-\hat{y})(y_{t+k}-\hat{y}) \equiv \frac{1}{N} \sum_{t=k+1}^{N} (y_{t-k}-\hat{y})(y_{t}-\hat{y}) \; (k=0,1,2,\ldots)$$

$$r_k = \frac{c_k}{c_0} \; (k=1,2,\ldots)$$

---

## ACF muestral

- La secuencia de valores numéricos $(r_k:k=1,2,\ldots)$ se denomina la **ACF muestral** de la serie temporal $y_1, y_2, \ldots, y_N$. La representación gráfica de la ACF muestral de una serie se denomina el **correlograma** de dicha serie.

- Bajo ciertas condiciones, $(\hat{\rho}_k) \sim NIID(0,1/N)$, de manera que cualquier autocorrelación simple $\rho_k$ $(k \geq 1)$ de $(Y_t)$ puede considerarse **individualmente** significativa al 5% cuando $|r_k| > 1.96 / \sqrt{N}$.

---

## Autocorrelación: Ejemplo (estacionalidad)

```{r fig.width=12, fig.height=7.5}
autoplot(window(ausbeer, start=1992)) + xlab("Año") + ylab("megalitros") +
  ggtitle("Producción trimestral de cerveza en Australia")
```


---

## Autocorrelación: Ejemplo (estacionalidad)

Resultados de los 9 primeros retardos de los datos de la producción de cerveza:



```{r, echo=FALSE}
beeracf <- matrix(acf(c(window(ausbeer, start=1992)), lag.max=9,
                      plot=FALSE)$acf[-1,,1], nrow=1)
colnames(beeracf) <- paste("r",1:9,sep="")
knitr::kable(beeracf, booktabs=TRUE, format = "html",
             align="c", digits=3, 
             format.args=list(nsmall=3))
```

```{r beeracf, fig.width=10, fig.height=5, fig.align='center'}
ggAcf(window(ausbeer, start=1992)) + labs(title = "Producción de cerveza en Australia")
```

---

## Autocorrelación: Ejemplo (estacionalidad)

  * $r_{4}$ más alto que para los otros retardos. Esto se debe al **patrón estacional de los datos**: los picos tienden a estar separados por **4 trimestres** y los mínimos tienden a estar separados por **2 trimestres**.
  * $r_2$ es más negativo que para los otros retardos porque los mínimos tienden a estar 2 trimestres por detrás de los máximos.
  * Juntos, las autocorrelaciones en los retardos $1, 2, \dots$, constituyen la **autocorrelación** o ACF.
  * El gráfico se conoce como **correlograma**.


```{r, fig.align='center', fig.width=10, fig.height=3.5}
ggAcf(window(ausbeer, start=1992)) + labs(title = "Producción de cerveza en Australia")
```
 
---


## Tendencia y estacionalidad en los gráficos ACF

- Cuando los datos tienen una tendencia, las autocorrelaciones para retardos pequeños tienden a ser grandes y positivas.

- Cuando los datos son estacionales, las autocorrelaciones serán mayores en los retardos estacionales (es decir, en los múltiplos de la frecuencia estacional)

- Cuando los datos tienen tendencia y son estacionales, se observa una combinación de estos efectos


---

## Autocorrelación: Ejemplo (tendencia)

### Precio de la acción de Google

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7}
autoplot(goog)  + labs(title = "Precio de la acción de Google")
```


---

## Autocorrelación: Ejemplo (tendencia)

### Precio de la acción de Google


```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6}
ggAcf(goog, lag.max=100) + labs(title = "Precio de la acción de Google")
```

- Cuando los datos tienen una tendencia, las autocorrelaciones para retardos pequeños tienden a ser grandes y positivas.

---

## Producción mensual de electricidad en Australia

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7}
elec2 <- window(elec, start=1980)
autoplot(elec2) + labs(title = "Producción mensual de electricidad en Australia")
```

---

## Producción mensual de electricidad en Australia

```{r , warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7}
ggAcf(elec2, lag.max=48) + labs(title = "Producción mensual de electricidad en Australia")
```

---

## Producción mensual de electricidad en Australia

El gráfico muestra una clara tendencia y estacionalidad.

Las mismas características se reflejan en el ACF.

  * El ACF, que decae lentamente, indica una tendencia.
  
  * Los picos del ACF en los retardos 12, 24 y 36, indican una estacionalidad de longitud 12.
  
---

## Proceso ruido blanco

- Ruido blanco $x_t = w_t$

- Donde $E(w_t)=0\;\;\; var(w_t)=\sigma^2 \;\;\; \forall t$

```{r ruido01, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7}

w = rnorm(500,0,1)

plot(w %>% ts, main="Ruido Blanco", xlab="Tiempo", ylab="Serie")

```


---

## Proceso ruido blanco

- Lo vemos con menos datos para que se visalize mejor

```{r ruido02, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

w1 = rnorm(100,0,1)

plot(w1 %>% ts, main="Ruido Blanco", xlab="Tiempo", ylab="Serie", type="b", col=2)

```


---

## Proceso ruido blanco

```{r, echo=FALSE}
wracf <- matrix(acf(c(w1), lag.max=9,
                      plot=FALSE)$acf[-1,,1], nrow=1)
colnames(wracf) <- paste("r",1:9,sep="")
knitr::kable(wracf, booktabs=TRUE,
             align="c", digits=3,
             format.args=list(nsmall=3))
```

```{r wnacfwarning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=4}
ggAcf(w1) + labs(title = "Proceso ruido blanco")
```

- Autocorrelación muestral para procesos ruido blanco 

- Esperamos que cada autocorrelación sea cercana a cero


---

## Proceso ruido blanco con deriva (drift)

- Ruido blanco con deriva $x_t =\delta +  w_t$

```{r ruido03, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

delta = 2
ww = w + delta
plot(ww %>% ts, main="Ruido Blanco con deriva", xlab="Tiempo", ylab="Serie", type="l", col=1)

```

---

## Proceso media movil

- Media móvil $x_t =  \theta w_t$

- En este caso particular el proceso es $x_t =\frac{1}{3} (w_{t-1}+w_t+w_{t+1})$. Se puede interpretar como una media centrada en cada observación


```{r ma01, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6.5}

v = stats::filter(w, sides=2, rep(1/3,3))

plot(v %>% ts, main="Media Móvil", xlab="Tiempo", ylab="Serie", type="l", col=1)

```

---

## Ambos procesos

```{r ma02, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

par(mfrow=c(2,1))
plot(w %>% ts, main="Ruido Blanco", xlab="Tiempo", ylab="Serie", type="l", col=1)
plot(v %>% ts, main="Media Móvil", xlab="Tiempo", ylab="Serie", type="l", col=2)

```

---

## Proceso media movil: ACF


```{r maacf, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6}
ggAcf(v) + labs(title = "Proceso media movil")
```

---


## Proceso media movil: ACF

$$x_t=w_t+0.5w_{t-1}+0.24w_{t-2}$$

```{r maacf02, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6}
b1 <- 0.5
b2 <- 0.24
v2 <- arima.sim(list(ma = c(b1,b2)), n=100, sd=2)
ggAcf(v2) + labs(title = "Proceso media movil")
```

---

## Proceso media movil: ACF

$$x_t=w_t+0.3w_{t-1}$$

```{r maacf03, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6}
v3 <- arima.sim(list(ma = c(0.3)), n=100, sd=2)
ggAcf(v3) + labs(title = "Proceso media movil")
```

---


## Procesos autoregresivos 

- Proceso autoregresivo de orden 2 $x_t = x_{t-1} - 0.9x_{t-2} + w_t$

```{r ar01, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

phi= -0.9          
z=rep(0,500)                                 
z[1]=0
z[2]=2
for (i in 3:500) z[i]=z[i-1]+phi*z[i-2]+w[i]   
plot(ts(z), main="Proceso autoregresivo", xlab="tiempo", ylab="Serie")  

```


---

## Procesos autoregresivos 

- Proceso autoregresivo de orden 2 $x_t = x_{t-1} - 0.9x_{t-2} + w_t$

```{r aracf, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6}
ggAcf(z) + labs(title = "Proceso autoregresivo")
```


---

## Procesos autoregresivos 

- Proceso autoregresivo de orden 2 $x_t = x_{t-1} - 0.9x_{t-2} + w_t$

```{r arpacf1, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6}
ggPacf(z) + labs(title = "PACF Proceso autoregresivo")
```


---

## Procesos autoregresivos 

- Proceso autoregresivo de orden 1 $x_t = 0.8x_{t-1} + w_t$

```{r ar02, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

phi= 0.8          
z1=rep(0,500)                                 
z1[1]=0
for (i in 3:500) z1[i]=phi*z1[i-1]+w[i]   
plot(ts(z1), main="Proceso autoregresivo orden 1", xlab="tiempo", ylab="Serie")  

```



---

## Procesos autoregresivos 

- Proceso autoregresivo de orden 1 $x_t = 0.8x_{t-1} + w_t$

```{r aracf02, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6}
ggAcf(z1) + labs(title = "Proceso autoregresivo orden 1")
```


---

## Procesos autoregresivos 

- Proceso autoregresivo de orden 2 $x_t = x_{t-1} - 0.9x_{t-2} + w_t$

```{r arpacf02, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6}
ggPacf(z1) + labs(title = "PACF Proceso autoregresivo orden 1")
```


---


## Random walk (Paseo aleatorio)

- Paseo aleatorio $x_t = x_{t-1} + w_t$

- Paseo aleatorio con deriva $x_t = \delta+ x_{t-1} + w_t$

```{r rw01, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=7.5}

set.seed(123)
TT <- 500
zz <- xx <- ww <- rnorm(n = TT, mean = 0, sd = 1)
for (t in 2:TT) {
  xx[t] <- xx[t - 1] + ww[t]
}
delta=0.2
for (t in 2:TT) {
  zz[t] <- delta + zz[t - 1] + ww[t]
}
plot(xx %>% ts, type="l", ylim=c(-20,100) ,ylab = "Series", xlab="Tiempo", main="Paseo Aleatorio")
lines(zz %>% ts)
lines(delta*(0:500), lty = "dashed", col=2)


```

---

## Estacionariedad


- Si $\{y_t\}$ es una serie temporal estacionaria, entonces para todo $s$, la distribución de $(y_t,\dots,y_{t+s})$ no depende de $t$.

- Las transformaciones ayudan a **estabilizar la varianza**.

- Para la modelización ARIMA, también necesitamos **estabilizar la media**.

---

## No estacionariedad en la media

Identificación de series no estacionarias


* Gráfico de la serie temporal.

* La ACF de los datos estacionarios cae a cero con relativa rapidez

* La ACF de los datos no estacionarios disminuye lentamente.

* Para los datos no estacionarios, el valor de $r_1$ es a menudo
     grande y positivo.
     
---

## Ejemplo: Indice Dow-Jones

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6, echo=FALSE}
autoplot(dj) + ylab("Indice Dow-Jones") + xlab("Día")
```


---

## Ejemplo: Indice Dow-Jones

```{r , warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6, echo=FALSE}
ggAcf(dj)
```

---

## Ejemplo: Indice Dow-Jones

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6, echo=FALSE}
autoplot(diff(dj)) + ylab("Variación del Indice Dow-Jones") + xlab("Día")
```

---

## Ejemplo: Indice Dow-Jones

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6, echo=FALSE}
ggAcf(diff(dj))
```

---

## Diferenciación

* La diferenciación ayuda a **estabilizar la media**.

* La serie diferenciada es el *cambio* entre cada observación
en la serie original: ${y'_t = y_t - y_{t-1}}$.

* La serie diferenciada sólo tendrá valores $T-1$ ya que no es posible calcular una diferencia $y_1'$ para la primera observación.

---

## Diferenciación de segundo orden

En ocasiones, los datos diferenciados no parecen estacionarios y puede ser necesario
puede ser necesario diferenciar los datos una segunda vez:

$$
\begin{aligned}
y''_{t}  &=  y'_{t}  - y'_{t - 1} \\
&= (y_t - y_{t-1}) - (y_{t-1}-y_{t-2})\\
&= y_t - 2y_{t-1} +y_{t-2}.
\end{aligned}
$$

* $y_t''$ tendrá $T-2$ valores.

* En la práctica, casi nunca es necesario ir más allá de las diferencias de segundo orden


---

## Diferencia estacional

Una diferencia estacional es la diferencia entre una observación y la observación correspondiente del año anterior.
$$y'_t = y_t - y_{t-m}$$
donde $m=$ número de estaciones

* Para datos mensuales $m=12$.

* Para datos trimestrales $m=4$.

---

## Producción de electricidad en Australia

Datos originales

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6, echo=FALSE}
usmelec %>% autoplot()
```

---

## Producción de electricidad en Australia

Aplicamos logaritmo

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6, echo=FALSE}
usmelec %>% log() %>% autoplot()
```

---

## Producción de electricidad en Australia

Aplicamos logaritmo + diferencia estacional (12 meses)

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6, echo=FALSE}
usmelec %>% log() %>% diff(lag=12) %>%
  autoplot()
```

---

## Producción de electricidad en Australia

Aplicamos logaritmo + diferencia estacional (12 meses) + diferencia regular (1)

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=12, fig.height=6, echo=FALSE}
usmelec %>% log() %>% diff(lag=12) %>%
  diff(lag=1) %>% autoplot()
```

---

## Producción de electricidad en Australia

* Las series diferenciadas estacionalmente están más cerca de ser estacionarias.

* La no estacionariedad restante puede ser eliminada con una primera diferencia adicional.

Si $y'_t = y_t - y_{t-12}$ denota una serie diferenciada estacionalmente, entonces la serie diferenciada dos veces 

$$
\begin{aligned}
y^*_t &= y'_t - y'_{t-1} \\
      &= (y_t - y_{t-12}) - (y_{t-1} - y_{t-13}) \\
      &= y_t - y_{t-1} - y_{t-12} + y_{t-13}\: .
\end{aligned}
$$

---

## Diferencias estacionales

Cuando se aplican tanto las diferencias estacionales como las primera diferencia

* no hay ninguna diferencia en cuál de las dos se haga primero: el resultado será el mismo.

* Si la estacionalidad es fuerte, recomendamos que se haga primero la diferenciación estacional porque a veces la serie resultante será estacionaria y no habrá necesidad de más primera diferencia.

Es importante que si se utiliza la diferenciación, las diferencias sean interpretables.


---

## Interpretación de las diferencias

* Las primeras diferencias son el cambio entre **una observación y la anterior**;

* las diferencias estacionales son el cambio entre **un año y el anterior**.


Pero si se toman las diferencias del tercer retardo para los datos anuales, por ejemplo, se obtiene un modelo que no puede interpretarse de forma razonable.